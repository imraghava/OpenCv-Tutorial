{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Understanding Features of an Image:\n",
    "\n",
    "### Why finding features in an image is important?\n",
    "<br>\n",
    "What if the computer can stitch several natural images to one, what about giving a lot of pictures of a building or any structure and tell computer to create a 3D model out of it? It is same as completing a jigsaw puzzle out of several images. How does human go about it?<br/>\n",
    "The answer is, we are looking for specific patterns or specific features which are unique, which can be easily tracked, which can be easily compared. If we go for a definition of such a feature, we may find it difficult to express it in words, but we know what are they. If some one asks you to point out one good feature which can be compared across several images, you can point out one. That is why, even small children can simply play these games. We search for these features in an image, we find them, we find the same features in other images, we align them. That’s it. (In jigsaw puzzle, we look more into continuity of different images). All these abilities are present in us inherently.\n",
    "**Features** are mainly some specific patterns in different part of the image that can be used for modelling.<br/>\n",
    "\n",
    "Consider a simple image(green part) below and the patches(red , blue , black) present on it:\n",
    "![title](81.png)\n",
    "\n",
    "Blue patch is flat area and difficult to find and track. Wherever you move the blue patch, it looks the same. For black patch, it is an edge. If you move it in vertical direction (i.e. along the gradient) it changes. Put along the edge (parallel to edge), it looks the same. And for red patch, it is a corner. Wherever you move the patch, it looks different, means it is unique. So basically, corners are considered to be good features in an image. (Not just corners, in some cases blobs are considered good features).\n",
    "\n",
    "### Feature detection:\n",
    "So now we answered our question, “what are these features?”. But next question arises. How do we find them? Or how do we find the corners?. That also we answered in an intuitive way, i.e., look for the regions in images which have maximum variation when moved (by a small amount) in all regions around it. This would be projected into computer language in coming chapters. So finding these image features is called **Feature Detection**.\n",
    "\n",
    "### Feature Description:\n",
    "So we found the features in image (Assume you did it). Once you found it, you should find the same in the other images. What we do? We take a region around the feature, we explain it in our own words, like “upper part is blue sky, lower part is building region, on that building there are some glasses etc” and you search for the same area in other images. Basically, you are describing the feature. Similar way, computer also should describe the region around the feature so that it can find it in other images. So called description is called **Feature Description**. Once you have the features and its description, you can find same features in all images and align them, stitch them or do whatever you want.\n",
    "\n",
    "### Why is a corner so special?\n",
    "Because, since it is the intersection of two edges, it represents a point in which the directions of these two edges change. Hence, the gradient of the image (in both directions) have a high variation, which can be used to detect it.\n",
    "\n",
    "\n",
    "Our aim is to find little patches of image (or \"windows\") that generate a large variation when moved around. Have a look at this image:\n",
    "![title](82.jpg)\n",
    "\n",
    "The red square is the window we've chosen. Moving it around doesn't show much of variation. That is, the difference between the window, and the original image below it is very low. So you can't really tell if the window \"belongs\" to that position.\n",
    "\n",
    "Of course, if you move the window too much, like onto the reddish region, you're bound to see a big difference. But we've moved the window too much. Not good.\n",
    "\n",
    "Now have a look at this:\n",
    "![title](83.jpg)\n",
    "\n",
    "See? Even the little movement of the window produces a noticeable difference. This is the kind of window we're looking for. Here's how it translates mathematically:\n",
    "![title](84.jpg)\n",
    "\n",
    "\n",
    "1. E is the difference between the original and the moved window.\n",
    "2. u is the window's displacement in the x direction\n",
    "3. v is the window's displacement in the y direction\n",
    "4. w(x, y) is the window at position (x, y). This acts like a mask. Ensuring that only the desired window is used.\n",
    "5. I is the intensity of the image at a position (x, y)\n",
    "6. I(x+u, y+v) is the intensity of the moved window\n",
    "7. I(x, y) is the intensity of the original\n",
    "\n",
    "\n",
    "We've looking for windows that produce a large E value. To do that, we need to high values of the terms inside the square brackets.\n",
    "\n",
    "So, we maximize this term:\n",
    "![title](85.jpg)\n",
    "\n",
    "Then, we expand this term using the Taylor series. Whats that? It's just a way of rewriting this term in using its derivatives.(It will be using Sobel Operator(introduced in previous tutorials) to find deriavtives.)\n",
    "\n",
    "![title](88.jpg)\n",
    "\n",
    "See how the I(x+u, y+v) changed into a totally different form ( I(x,y)+uIx + vIy )? Thats the Taylor series in action. And because the Taylor series is infinite, we've ignored all terms after the first three. It gives a pretty good approximation. But it isn't the actual value.\n",
    "\n",
    "Next, we expand the square. The I(x,y) cancels out, so its just two terms we need to square. It looks like this:\n",
    "![title](86.jpg)\n",
    "\n",
    "Now this messy equation can be tucked up into a neat little matrix form like this:\n",
    "![title](87.jpg)\n",
    "\n",
    "Now, we rename the summed-matrix, and put it to be M:\n",
    "![title](89.jpg)\n",
    "\n",
    "So the equation now becomes:\n",
    "![title](90.jpg)\n",
    "\n",
    "\n",
    "#### Then comes the main part. After this, they created a score R, basically an equation, which will determine if a window can contain a corner or not.\n",
    "![title](91.jpg)\n",
    "\n",
    "![title](92.png)\n",
    "\n",
    "\n",
    "#### So the result of Harris Corner Detection is a grayscale image with these scores. Thresholding for a suitable give you the corners in the image. We will do it with a simple image.\n",
    "\n",
    "\n",
    "OpenCV has the function **cv2.cornerHarris()** for this purpose. Its arguments are :\n",
    "\n",
    "1. **img** - Input image, it should be grayscale and float32 type.\n",
    "2. **blockSize** - It is the size of neighbourhood considered for corner detection (size of window)\n",
    "3. **ksize** - Aperture parameter of Sobel derivative used.\n",
    "4. **k** - Harris detector free parameter in the equation.\n",
    "\n",
    "The Harris Corner Detector is just a mathematical way of determining which windows produce large variations when moved in any direction. With each window, a score R is associated. Based on this score, you can figure out which ones are corners and which ones are not.\n",
    "<br/>\n",
    "### Example: Performing corner detection on two images (first is more symmetric than the second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "img1 = cv2.imread(\"93.jpg\")\n",
    "img2 = cv2.imread(\"95.jpeg\")\n",
    "print(\"Image1 Matrix Shape-\" , img1.shape) \n",
    "gray1 = cv2.cvtColor(img1 , cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2 , cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "gray1 = np.float32(gray1) #Harris corner detector takes up float values\n",
    "gray2 = np.float32(gray2)\n",
    "print(\"Gray1 Image Matrix Shape-\" , gray1.shape) \n",
    "result1 = cv2.cornerHarris(gray1 , 2 , 3 , 0.04) #img , bloacksize , ksize , k\n",
    "result2 = cv2.cornerHarris(gray2 , 2 , 3 , 0.04)\n",
    "print(\"Reasulting(score) Image1 Matrix Shape-\" , result1.shape)\n",
    "\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "img1[result1 > 0.01*result1.max()] = [0 , 0 , 255] \n",
    "img2[result2 > 0.01*result2.max()] = [255 , 0 , 0] \n",
    "\n",
    "#The result matrix contains the score of each pixel being a corner or not.\n",
    "#Most of the pixel have negative values, edges have small positive values or negative values \n",
    "#and corners have high positive values \n",
    "#Maximum value in the result matrix = 22283716.00\n",
    "#We are filtering out those values of pixels in result matrix which are more than one percent of the maximum value\n",
    "#assuming them to be corners and are putting red color value(0 , 0 255) on those pixels in the image matrix.\n",
    "#We can experiment with threshold value.\n",
    "\n",
    "while(True):\n",
    "    cv2.imshow(\"img1\" , img1)\n",
    "    cv2.imshow(\"img2\" , img2)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "for i in range(5):\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Original Image1\n",
    "![title](93.jpg)\n",
    "## Resulting Image1\n",
    "![title](94.png)\n",
    "## Original Image1\n",
    "![title](95.jpeg)\n",
    "## Resulting Image1 (when threshold is 1% of the max value of score)\n",
    "![title](96.png)\n",
    "## Resulting Image1 (when threshold is 0.1% of the max value of score)\n",
    "![title](97.png)\n",
    "## Resulting Image1 (when threshold is 0.01% of the max value of score)\n",
    "### In this case it is detecting a few edges too as corners\n",
    "![title](98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Shi-Tomasi Corner Detector & Good Features to Track\n",
    "\n",
    "### Function = cv2.goodFeaturesToTrack()\n",
    "\n",
    "in 1994, J. Shi and C. Tomasi made a small modification to it in their paper Good Features to Track which shows better results compared to Harris Corner Detector. The scoring function in Harris Corner Detector was given by:\n",
    "![title](91.jpg)\n",
    "\n",
    "Instead of this, Shi-Tomasi proposed:\n",
    "\n",
    "![title](99.png)\n",
    "\n",
    "OpenCV has a function, **cv2.goodFeaturesToTrack()**. It finds N strongest corners in the image by Shi-Tomasi method (or Harris Corner Detection, if you specify it). As usual, image should be a grayscale image. Then you specify number of corners you want to find. Then you specify the quality level, which is a value between 0-1, which denotes the minimum quality of corner below which everyone is rejected. Then we provide the minimum euclidean distance between corners detected.\n",
    "\n",
    "With all these informations, the function finds corners in the image. All corners below quality level are rejected. Then it sorts the remaining corners based on quality in the descending order. Then function takes first strongest corner, throws away all the nearby corners in the range of minimum distance and returns N strongest corners.\n",
    "\n",
    "In below example, we will try to find 25 best corners:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = imread(\"95.jpeg\")\n",
    "gray = cv2.cvtColor( img , cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
