{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template Matching is a method for searching and finding the location of a template image in a larger image. OpenCV comes with a function cv2.matchTemplate() for this purpose. It simply slides the template image over the input image (as in 2D convolution) and compares the template and patch of input image under the template image. Several comparison methods are implemented in OpenCV. (You can check docs for more details). It returns a grayscale image, where each pixel denotes how much does the neighbourhood of that pixel match with template.\n",
    "\n",
    "\n",
    "We need two primary components: <br/>\n",
    "1. **Source image (I):** The image in which we expect to find a match to the template image <br/>\n",
    "2. **Template image (T):** The patch image which will be compared to the template image <br/>\n",
    "\n",
    "**Source Image**\n",
    "![title](57.png)\n",
    "\n",
    "**Template Image**\n",
    "![title](58.png)\n",
    "\n",
    "Our goal is to indentify the highest matching area.\n",
    "\n",
    "To identify the matching area, we have to compare the template image against the source image by sliding it: <br/>\n",
    "By sliding, we mean moving the patch one pixel at a time (left to right, up to down). At each location, a metric is calculated so it represents how “good” or “bad” the match at that location is (or how similar the patch is to that particular area of the source image). <br/>\n",
    "<br/>\n",
    "**Mathematics behind template matching:**\n",
    "A basic method of template matching uses an image patch (template), tailored to a specific feature of the search image, which we want to detect. This technique can be easily performed on grey images or edge images. The cross correlation output will be highest at places where the image structure matches the mask structure, where large image values get multiplied by large mask values.\n",
    "\n",
    "This method is normally implemented by first picking out a part of the search image to use as a template: We will call the search image **S(x, y)**, where **(x, y)** represent the coordinates of each pixel in the search image. We will call the template **T(xt, yt)**, where **(xt, yt)** represent the coordinates of each pixel in the template. We then simply move the center (or the origin) of the template **T(xt, yt)** over each **(x, y)** point in the search image and calculate the sum of products between the coefficients in **S(x, y)** and **T(xt, yt)** over the whole area spanned by the template. As all possible positions of the template with respect to the search image are considered, the position with the highest score is the best position. This method is sometimes referred to as **'Linear Spatial Filtering'** and the template is called a filter mask.\n",
    "\n",
    "Another way to handle translation problems on images using template matching is to compare the intensities of the pixels, using the **SAD (Sum of absolute differences)** measure.\n",
    "\n",
    "A pixel in the search image with coordinates **(xs, ys)** has intensity **Is(xs, ys)** and a pixel in the template with coordinates **(xt, yt)** has intensity **It(xt, yt )**. Thus the absolute difference in the pixel intensities is defined as **Diff(xs, ys, xt, yt) = | Is(xs, ys) – It(xt, yt) |.**\n",
    "\n",
    "![title](55.svg)\n",
    "\n",
    "The mathematical representation of the idea about looping through the pixels in the search image as we translate the origin of the template at every pixel and take the SAD measure is the following:\n",
    "\n",
    "\n",
    "![title](56.svg)\n",
    "\n",
    "**Srows** and **Scols** denote the rows and the columns of the search image and Trows and Tcols denote the rows and the columns of the template image, respectively. In this method the lowest **SAD** score gives the estimate for the best position of template within the search image. The method is simple to implement and understand, but it is one of the slowest methods.\n",
    "\n",
    "\n",
    "\n",
    "**Limitation:**\n",
    "The template is only one copy of the different versions of the same image. Ideally, it should match with every image of the object, but this can't be achieved through template matching as it has only one copy.\n",
    "\n",
    "**Use:**\n",
    "Template matching is useful in the cases of GUI, as the file button on the top will remain the same in every GUI and there are no different versions of it.\n",
    "Template matching is useful where we need above 80%(very close) with the original image.\n",
    "\n",
    "## Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_bgr = cv2.imread(\"57.png\") \n",
    "img_gray = cv2.cvtColor(img_bgr , cv2.COLOR_BGR2GRAY) \n",
    "# converted to gray image for easier detection, channels reduced to 1 \n",
    "\n",
    "\n",
    "\n",
    "template = cv2.imread(\"58.png\" , 0) \n",
    "#Loading template\n",
    "\n",
    "\n",
    "\n",
    "w , h = template.shape[: : -1]  \n",
    "#[: : -1] is used for flipping to (width,height) dimension from (height, width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = cv2.matchTemplate(img_gray , template , cv2.TM_CCOEFF_NORMED) \n",
    "#\"result\" matrix by sliding template matrix over image matrix and applying the type of match operation to be used\n",
    "\n",
    "\n",
    "threshold = 0.8 \n",
    "#threshold for matching(80% here)\n",
    "\n",
    "location = np.where(result >= threshold) \n",
    "#taking all the pixels where result>= threshold value\n",
    "\n",
    "\n",
    "#we are considering here that each point of the location matrix is the left most point and is starting point of the\n",
    "#of the detected area.\n",
    "\n",
    "\n",
    "\n",
    "for pt in zip(* location[: : -1]): \n",
    "    #For each point in the location matrix we draw a rectangle of the size of the template to show the detected object . \n",
    "    \n",
    "    cv2.rectangle(img_bgr , pt , (pt[0]+w , pt[1]+h) , (0, 255 , 255) , 1)\n",
    "    #arguements = (image , starting point , (width , height of the rectangle) , color of the line , width of the line)\n",
    "\n",
    "    \n",
    "#Displaying the image\n",
    "while True:\n",
    "    cv2.imshow('detected' , img_bgr)\n",
    "    \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "for i in range(5):\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "The output for the different values of the threshold are shown:\n",
    "\n",
    "1. **Threshold = 0.4**\n",
    "\n",
    "![title](0.4.png)\n",
    "\n",
    "2. **Threshold = 0.45**\n",
    "\n",
    "![title](0.45.png)\n",
    "\n",
    "3. **Threshold = 0.5**\n",
    "\n",
    "![title](0.5.png)\n",
    "\n",
    "\n",
    "4. **Threshold = 0.6**\n",
    "\n",
    "![title](0.6.png)\n",
    "\n",
    "5. **Threshold = 0.7**\n",
    "\n",
    "![title](0.7.png)\n",
    "\n",
    "\n",
    "**We have used here the TF_CCOEFF_NORMED method for matching. There are other methods too which uses different formulas for calculation of the result matrix using sliding.** <br/>\n",
    "\n",
    "**Here I is the Source Image and T is the Template Image**\n",
    "\n",
    "\n",
    "\n",
    "![title](60.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
